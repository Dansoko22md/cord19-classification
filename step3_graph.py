import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
import pickle
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Essayer d'importer community louvain
try:
    import community.community_louvain as community_louvain
    HAS_LOUVAIN = True
except ImportError:
    print("‚ö†Ô∏è  python-louvain non install√©, utilisation de l'algorithme greedy")
    HAS_LOUVAIN = False

class ArticleGraphBuilder:
    """Classe pour construire un graphe d'articles scientifiques"""
    
    def __init__(self, df, embeddings):
        """
        Initialise le constructeur de graphe
        
        Args:
            df: DataFrame contenant les m√©tadonn√©es des articles
            embeddings: Array numpy des embeddings
        """
        self.df = df.reset_index(drop=True)
        self.embeddings = embeddings
        self.graph = None
        self.communities = None
        
        print(f"üîß Initialisation du constructeur de graphe")
        print(f"   - {len(df)} articles")
        print(f"   - Embeddings: {embeddings.shape}")
    
    def build_similarity_graph(self, threshold=0.7, max_edges_per_node=10):
        """
        Construit un graphe bas√© sur la similarit√© cosinus
        
        Args:
            threshold: Seuil de similarit√© minimum pour cr√©er une ar√™te
            max_edges_per_node: Nombre maximum de connexions par n≈ìud
            
        Returns:
            NetworkX Graph
        """
        print(f"\nüèóÔ∏è  Construction du graphe de similarit√©...")
        print(f"   - Seuil: {threshold}")
        print(f"   - Max ar√™tes/n≈ìud: {max_edges_per_node}")
        
        # Cr√©er le graphe
        G = nx.Graph()
        
        # Ajouter les n≈ìuds avec attributs
        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc="Ajout des n≈ìuds"):
            G.add_node(idx, 
                      title=str(row['title'])[:100] if pd.notna(row['title']) else 'No title',
                      abstract=str(row['abstract'])[:200] if pd.notna(row['abstract']) else '',
                      source=str(row['source_x']) if 'source_x' in row else 'unknown')
        
        # Calculer la similarit√© cosinus
        print("\n   üìä Calcul de la matrice de similarit√©...")
        similarity_matrix = cosine_similarity(self.embeddings)
        
        # Ajouter les ar√™tes
        print("   üîó Ajout des ar√™tes...")
        edge_count = 0
        
        for i in tqdm(range(len(similarity_matrix)), desc="Cr√©ation des liens"):
            # Obtenir les indices tri√©s par similarit√© d√©croissante
            similarities = similarity_matrix[i]
            sorted_indices = np.argsort(similarities)[::-1]
            
            # Prendre les top k voisins (en excluant le n≈ìud lui-m√™me)
            count = 0
            for j in sorted_indices[1:]:  # Exclure l'article lui-m√™me
                sim_score = similarities[j]
                
                if sim_score >= threshold and count < max_edges_per_node:
                    if not G.has_edge(i, j):
                        G.add_edge(i, j, weight=float(sim_score))
                        edge_count += 1
                        count += 1
                elif sim_score < threshold:
                    break  # Arr√™ter si en dessous du seuil
        
        self.graph = G
        
        print(f"\n‚úÖ Graphe construit:")
        print(f"   - N≈ìuds: {G.number_of_nodes():,}")
        print(f"   - Ar√™tes: {G.number_of_edges():,}")
        print(f"   - Densit√©: {nx.density(G):.6f}")
        print(f"   - Composantes connexes: {nx.number_connected_components(G)}")
        
        return G
    
    def detect_communities(self, algorithm='louvain'):
        """
        D√©tecte les communaut√©s (clusters th√©matiques)
        
        Args:
            algorithm: 'louvain' ou 'greedy'
            
        Returns:
            Dictionnaire {node: community_id}
        """
        print(f"\nüîç D√©tection de communaut√©s ({algorithm})...")
        
        if self.graph is None:
            raise ValueError("Le graphe doit √™tre construit d'abord")
        
        # Utiliser le plus grand composant connect√©
        if nx.number_connected_components(self.graph) > 1:
            largest_cc = max(nx.connected_components(self.graph), key=len)
            G_connected = self.graph.subgraph(largest_cc).copy()
            print(f"   ‚ö†Ô∏è  Utilisation du plus grand composant: {len(G_connected)} n≈ìuds")
        else:
            G_connected = self.graph
        
        if algorithm == 'louvain' and HAS_LOUVAIN:
            self.communities = community_louvain.best_partition(G_connected)
            modularity = community_louvain.modularity(self.communities, G_connected)
        else:
            # Algorithme greedy (backup)
            if not HAS_LOUVAIN:
                print("   ‚ö†Ô∏è  Louvain non disponible, utilisation de greedy")
            communities_gen = nx.community.greedy_modularity_communities(G_connected)
            self.communities = {}
            for i, comm in enumerate(communities_gen):
                for node in comm:
                    self.communities[node] = i
            modularity = nx.community.modularity(G_connected, communities_gen)
        
        # Ajouter les n≈ìuds isol√©s (communaut√© -1)
        for node in self.graph.nodes():
            if node not in self.communities:
                self.communities[node] = -1
        
        # Statistiques
        num_communities = len(set(self.communities.values()))
        
        print(f"‚úÖ Communaut√©s d√©tect√©es:")
        print(f"   - Nombre: {num_communities}")
        print(f"   - Modularit√©: {modularity:.4f}")
        
        # Distribution des tailles
        comm_sizes = pd.Series(self.communities.values()).value_counts().sort_index()
        print(f"\nüìä Distribution des tailles:")
        print(f"   - Moyenne: {comm_sizes.mean():.1f} articles/cluster")
        print(f"   - M√©diane: {comm_sizes.median():.1f} articles/cluster")
        print(f"   - Min: {comm_sizes.min()} articles/cluster")
        print(f"   - Max: {comm_sizes.max()} articles/cluster")
        
        return self.communities
    
    def analyze_communities(self, top_n=5):
        """
        Analyse les communaut√©s d√©tect√©es
        
        Args:
            top_n: Nombre de top communaut√©s √† analyser
            
        Returns:
            DataFrame avec les statistiques des communaut√©s
        """
        print(f"\nüìä ANALYSE DES COMMUNAUT√âS")
        print("="*60)
        
        if self.communities is None:
            raise ValueError("Les communaut√©s doivent √™tre d√©tect√©es d'abord")
        
        # Ajouter les communaut√©s au DataFrame
        self.df['community'] = self.df.index.map(self.communities)
        
        # Statistiques par communaut√©
        comm_stats = []
        
        for comm_id in sorted(set(self.communities.values())):
            if comm_id == -1:  # Ignorer les isol√©s
                continue
                
            articles = self.df[self.df['community'] == comm_id]
            
            comm_stats.append({
                'community_id': comm_id,
                'size': len(articles),
                'avg_connections': np.mean([self.graph.degree(node) 
                                           for node in articles.index if node in self.graph]),
                'sample_titles': articles['title'].head(3).tolist()
            })
        
        stats_df = pd.DataFrame(comm_stats).sort_values('size', ascending=False)
        
        # Afficher les top communaut√©s
        print(f"\nüèÜ Top {top_n} communaut√©s par taille:")
        for i, row in stats_df.head(top_n).iterrows():
            print(f"\n   Communaut√© {row['community_id']}:")
            print(f"   - Taille: {row['size']} articles")
            print(f"   - Connexions moyennes: {row['avg_connections']:.1f}")
            print(f"   - Exemples de titres:")
            for j, title in enumerate(row['sample_titles'], 1):
                title_str = str(title)[:80] if pd.notna(title) else 'No title'
                print(f"      {j}. {title_str}...")
        
        return stats_df
    
    def visualize_graph(self, max_nodes=500, layout='spring'):
        """
        Visualise le graphe avec les communaut√©s
        
        Args:
            max_nodes: Nombre maximum de n≈ìuds √† afficher
            layout: Type de layout ('spring', 'kamada_kawai', 'circular')
        """
        print(f"\nüé® Visualisation du graphe...")
        
        if self.graph is None:
            raise ValueError("Le graphe doit √™tre construit d'abord")
        
        # Prendre un sous-graphe si trop grand
        if self.graph.number_of_nodes() > max_nodes:
            print(f"   ‚ö†Ô∏è  Graphe trop grand, affichage d'un √©chantillon de {max_nodes} n≈ìuds")
            # Prendre les n≈ìuds avec le plus de connexions
            degrees = dict(self.graph.degree())
            top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:max_nodes]
            G_viz = self.graph.subgraph(top_nodes).copy()
        else:
            G_viz = self.graph
        
        # Configuration de la figure
        fig, axes = plt.subplots(1, 2, figsize=(20, 10))
        
        # Layout
        print(f"   üéØ Calcul du layout ({layout})...")
        if layout == 'spring':
            pos = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)
        elif layout == 'kamada_kawai':
            pos = nx.kamada_kawai_layout(G_viz)
        elif layout == 'circular':
            pos = nx.circular_layout(G_viz)
        else:
            pos = nx.spring_layout(G_viz, seed=42)
        
        # Graphe 1: Sans couleurs de communaut√©s
        ax1 = axes[0]
        nx.draw_networkx_nodes(G_viz, pos, node_size=30, alpha=0.6, 
                              node_color='steelblue', ax=ax1)
        nx.draw_networkx_edges(G_viz, pos, alpha=0.2, width=0.5, ax=ax1)
        ax1.set_title(f"Graphe d'Articles (Structure)\n{G_viz.number_of_nodes()} n≈ìuds, "
                     f"{G_viz.number_of_edges()} ar√™tes", fontsize=14)
        ax1.axis('off')
        
        # Graphe 2: Avec couleurs de communaut√©s
        if self.communities is not None:
            ax2 = axes[1]
            
            # Couleurs des communaut√©s
            node_colors = [self.communities.get(node, 0) for node in G_viz.nodes()]
            
            nx.draw_networkx_nodes(G_viz, pos, node_size=30, alpha=0.7,
                                  node_color=node_colors, cmap='tab20', ax=ax2)
            nx.draw_networkx_edges(G_viz, pos, alpha=0.2, width=0.5, ax=ax2)
            
            ax2.set_title(f"Graphe avec Communaut√©s Th√©matiques\n"
                         f"{len(set(self.communities.values()))} clusters d√©tect√©s",
                         fontsize=14)
            ax2.axis('off')
        
        plt.tight_layout()
        plt.savefig('graph_visualization.png', dpi=300, bbox_inches='tight')
        print(f"   üìä Visualisation sauvegard√©e: graph_visualization.png")
        plt.show()
    
    def get_graph_statistics(self):
        """Calcule et affiche les statistiques du graphe"""
        print(f"\nüìà STATISTIQUES DU GRAPHE")
        print("="*60)
        
        if self.graph is None:
            raise ValueError("Le graphe doit √™tre construit d'abord")
        
        G = self.graph
        
        # Statistiques de base
        print(f"\nüî¢ M√©triques de base:")
        print(f"   - N≈ìuds: {G.number_of_nodes():,}")
        print(f"   - Ar√™tes: {G.number_of_edges():,}")
        print(f"   - Densit√©: {nx.density(G):.6f}")
        
        # Degr√©s
        degrees = [d for n, d in G.degree()]
        print(f"\nüìä Distribution des degr√©s:")
        print(f"   - Degr√© moyen: {np.mean(degrees):.2f}")
        print(f"   - Degr√© m√©dian: {np.median(degrees):.0f}")
        print(f"   - Degr√© max: {max(degrees)}")
        print(f"   - Degr√© min: {min(degrees)}")
        
        # Composantes connexes
        components = list(nx.connected_components(G))
        print(f"\nüîó Connectivit√©:")
        print(f"   - Composantes connexes: {len(components)}")
        print(f"   - Taille plus grande composante: {len(max(components, key=len))}")
        
        # Centralit√© (sur un √©chantillon si trop grand)
        if G.number_of_nodes() < 1000:
            print(f"\n‚≠ê Centralit√©:")
            degree_cent = nx.degree_centrality(G)
            top_central = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:5]
            print(f"   Top 5 n≈ìuds centraux:")
            for node, cent in top_central:
                title = str(self.df.iloc[node]['title'])[:60] if pd.notna(self.df.iloc[node]['title']) else 'No title'
                print(f"      - {title}... (centralit√©: {cent:.4f})")
        
        # Modularit√©
        if self.communities is not None and HAS_LOUVAIN:
            # Calculer sur le plus grand composant
            largest_cc = max(nx.connected_components(G), key=len)
            G_connected = G.subgraph(largest_cc).copy()
            communities_connected = {k: v for k, v in self.communities.items() if k in G_connected}
            modularity = community_louvain.modularity(communities_connected, G_connected)
            print(f"\nüéØ Qualit√© des clusters:")
            print(f"   - Modularit√©: {modularity:.4f}")
        
        return {
            'num_nodes': G.number_of_nodes(),
            'num_edges': G.number_of_edges(),
            'density': nx.density(G),
            'avg_degree': np.mean(degrees),
            'num_communities': len(set(self.communities.values())) if self.communities else None
        }
    
    def save_graph(self, path):
        """Sauvegarde le graphe et les communaut√©s"""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarder le graphe
        nx.write_gpickle(self.graph, path)
        
        # Sauvegarder les communaut√©s
        if self.communities is not None:
            comm_path = path.parent / (path.stem + '_communities.pkl')
            with open(comm_path, 'wb') as f:
                pickle.dump(self.communities, f)
        
        print(f"\nüíæ Graphe sauvegard√©: {path}")
    
    def export_for_gephi(self, path):
        """Exporte le graphe au format GEXF pour Gephi"""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        # Ajouter les attributs de communaut√©
        if self.communities is not None:
            for node in self.graph.nodes():
                self.graph.nodes[node]['community'] = self.communities.get(node, -1)
        
        nx.write_gexf(self.graph, path)
        print(f"\nüíæ Graphe export√© pour Gephi: {path}")


# EXEMPLE D'UTILISATION
if __name__ == "__main__":
    print("="*60)
    print("PHASE 3: CONSTRUCTION DU GRAPHE D'ARTICLES")
    print("="*60)
    
    # 1. Charger les donn√©es et embeddings
    data_path = Path("S1_CORD19_Classification/data/processed/cleaned_articles.csv")
    embeddings_path = Path("S1_CORD19_Classification/data/processed/embeddings.npy")
    
    print(f"\nüìÇ Chargement des donn√©es...")
    df = pd.read_csv(data_path)
    embeddings = np.load(embeddings_path)
    
    print(f"   ‚úÖ Articles CSV: {len(df):,}")
    print(f"   ‚úÖ Embeddings: {embeddings.shape}")
    
    # V√©rifier la coh√©rence
    if len(df) != len(embeddings):
        print(f"\n‚ö†Ô∏è  ATTENTION: Incoh√©rence d√©tect√©e!")
        print(f"   Articles: {len(df)}, Embeddings: {len(embeddings)}")
        print(f"   Alignement sur le minimum...")
        min_size = min(len(df), len(embeddings))
        df = df.iloc[:min_size].reset_index(drop=True)
        embeddings = embeddings[:min_size]
        print(f"   ‚úÖ Align√© sur {min_size} articles")
    
    # √âchantillon pour test rapide
    SAMPLE_SIZE = 5000  # Mettre None pour traiter tout
    if SAMPLE_SIZE and len(df) > SAMPLE_SIZE:
        print(f"\n‚ö†Ô∏è  Mode √©chantillon: {SAMPLE_SIZE} articles")
        indices = np.random.choice(len(df), SAMPLE_SIZE, replace=False)
        df = df.iloc[indices].reset_index(drop=True)
        embeddings = embeddings[indices]
    
    # 2. Construire le graphe
    builder = ArticleGraphBuilder(df, embeddings)
    
    # Param√®tres du graphe
    graph = builder.build_similarity_graph(
        threshold=0.75,  # Seuil de similarit√©
        max_edges_per_node=15  # Connexions max par article
    )
    
    # 3. D√©tecter les communaut√©s
    communities = builder.detect_communities(algorithm='louvain')
    
    # 4. Analyser les communaut√©s
    stats = builder.analyze_communities(top_n=10)
    
    # 5. Obtenir les statistiques
    graph_stats = builder.get_graph_statistics()
    
    # 6. Visualiser
    builder.visualize_graph(max_nodes=500, layout='spring')
    
    # 7. Sauvegarder
    output_dir = Path("S1_CORD19_Classification/data/processed")
    builder.save_graph(output_dir / "article_graph.gpickle")
    builder.export_for_gephi(output_dir / "article_graph.gexf")
    
    # 8. Sauvegarder le DataFrame avec communaut√©s
    df_with_comm = builder.df.copy()
    df_with_comm.to_csv(output_dir / "articles_with_communities.csv", index=False)
    
    print("\n" + "="*60)
    print("‚úÖ PHASE 3 TERMIN√âE!")
    print("="*60)
    print(f"\nüì¶ R√©sum√©:")
    print(f"   - Graphe: {graph.number_of_nodes():,} n≈ìuds, {graph.number_of_edges():,} ar√™tes")
    print(f"   - Communaut√©s: {len(set(communities.values()))}")
    print(f"   - Fichiers sauvegard√©s dans: {output_dir}")
    print(f"\nüéØ Fichiers g√©n√©r√©s:")
    print(f"   - article_graph.gpickle : Graphe NetworkX")
    print(f"   - article_graph.gexf : Pour Gephi")
    print(f"   - articles_with_communities.csv : Articles avec clusters")
    print(f"   - graph_visualization.png : Visualisation")
    print(f"\n‚û°Ô∏è  Prochaine √©tape: Analyse approfondie ou Graph-RAG")